{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import logging\n",
    "from typing import Dict, Optional, List\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('maritime_classifier.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class MaritimeRiskClassifier:\n",
    "    def __init__(self, model_name: str = \"cross-encoder/nli-distilroberta-base\"):\n",
    "        \"\"\"\n",
    "        Initialize the Maritime Risk Classifier with improved error handling and logging.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the transformer model to use\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(\"Initializing Maritime Risk Classifier\")\n",
    "\n",
    "        # Initialize device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.logger.info(f\"Using device: {self.device}\")\n",
    "\n",
    "        try:\n",
    "            self.classifier = pipeline(\n",
    "                \"zero-shot-classification\",\n",
    "                model=model_name,\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "            self.logger.info(\"Successfully loaded classification model\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Enhanced severity levels with more detailed descriptions\n",
    "        self.severity_levels = {\n",
    "            1: \"Low - Minimal impacts\",\n",
    "            2: \"Moderate - Some disruption\",\n",
    "            3: \"High - Significant operational impact; may involve injuries\",\n",
    "            4: \"Critical - Major incident with severe impact, likely with injuries\",\n",
    "            5: \"Catastrophic - Severe consequences, likely fatalities or major environmental damage\"\n",
    "        }\n",
    "\n",
    "        self.candidate_labels = list(self.severity_levels.values())\n",
    "\n",
    "    def clean_input(self, text: Optional[str]) -> str:\n",
    "        \"\"\"\n",
    "        Clean and validate input text with improved handling.\n",
    "\n",
    "        Args:\n",
    "            text: Input text to clean\n",
    "\n",
    "        Returns:\n",
    "            str: Cleaned text\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text is None:\n",
    "            return \"\"\n",
    "        return str(text).strip()\n",
    "\n",
    "    def predict(self, headline: str, description: str, risk_type: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Predict severity level for a maritime incident with enhanced error handling.\n",
    "\n",
    "        Args:\n",
    "            headline: Incident headline\n",
    "            description: Incident description\n",
    "            risk_type: Type of risk\n",
    "\n",
    "        Returns:\n",
    "            Dict containing severity level, label, and confidence\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Clean inputs\n",
    "            headline = self.clean_input(headline)\n",
    "            description = self.clean_input(description)\n",
    "            risk_type = self.clean_input(risk_type)\n",
    "\n",
    "            # Combine inputs with weights\n",
    "            desc_text = description[:200] if description else \"\"\n",
    "            text = f\"{headline} | {risk_type} | {desc_text}\".strip()\n",
    "\n",
    "            if not text.strip():\n",
    "                self.logger.warning(\"Empty input received\")\n",
    "                return self._get_default_prediction()\n",
    "\n",
    "            # Get prediction\n",
    "            result = self.classifier(text, self.candidate_labels)\n",
    "\n",
    "            # Extract results\n",
    "            predicted_label = result['labels'][0]\n",
    "            confidence = result['scores'][0]\n",
    "            severity_level = self._get_severity_level(predicted_label)\n",
    "\n",
    "            return {\n",
    "                'severity_level': severity_level,\n",
    "                'severity_label': predicted_label,\n",
    "                'confidence': confidence,\n",
    "                'input_text': text\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Prediction error: {str(e)}\")\n",
    "            return self._get_default_prediction()\n",
    "\n",
    "    def _get_severity_level(self, label: str) -> int:\n",
    "        \"\"\"Extract severity level from label.\"\"\"\n",
    "        severity_mapping = {\n",
    "            \"Low\": 1,\n",
    "            \"Moderate\": 2,\n",
    "            \"High\": 3,\n",
    "            \"Critical\": 4,\n",
    "            \"Catastrophic\": 5\n",
    "        }\n",
    "        return severity_mapping[label.split(' -')[0]]\n",
    "\n",
    "    def _get_default_prediction(self) -> Dict:\n",
    "        \"\"\"Return default prediction for error cases.\"\"\"\n",
    "        return {\n",
    "            'severity_level': 1,\n",
    "            'severity_label': self.severity_levels[1],\n",
    "            'confidence': 0.0,\n",
    "            'input_text': ''\n",
    "        }\n",
    "\n",
    "    def classify_dataset(self, df: pd.DataFrame, batch_size: int = 100) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Classify a dataset of maritime incidents with progress tracking and batching.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            batch_size: Size of batches for processing\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with classification results\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting classification of {len(df)} records\")\n",
    "        results = []\n",
    "\n",
    "        # Clean DataFrame\n",
    "        df = df.fillna('')\n",
    "\n",
    "        try:\n",
    "            # Use tqdm for progress tracking\n",
    "            for idx in tqdm(range(len(df)), desc=\"Processing records\"):\n",
    "                row = df.iloc[idx]\n",
    "                try:\n",
    "                    prediction = self.predict(\n",
    "                        str(row.get('Cleaned_Headline', '')),\n",
    "                        str(row.get('Cleaned_Description', '')),\n",
    "                        str(row.get('Final Classification', ''))\n",
    "                    )\n",
    "\n",
    "                    results.append({\n",
    "                        'headline': row.get('Cleaned_Headline', ''),\n",
    "                        'Final Classification': row.get('Final Classification', ''),\n",
    "                        'severity_level': prediction['severity_level'],\n",
    "                        'severity_label': prediction['severity_label'],\n",
    "                        'confidence': prediction['confidence'],\n",
    "                        'input_text': prediction['input_text']\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing row {idx}: {str(e)}\")\n",
    "                    results.append(self._get_error_row(row))\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in classify_dataset: {str(e)}\")\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def _get_error_row(self, row: pd.Series) -> Dict:\n",
    "        \"\"\"Create error row for failed classifications.\"\"\"\n",
    "        return {\n",
    "            'headline': row.get('Cleaned_Headline', ''),\n",
    "            'Final Classification': row.get('Final Classification', ''),\n",
    "            'severity_level': 1,\n",
    "            'severity_label': 'Error in processing',\n",
    "            'confidence': 0.0,\n",
    "            'input_text': ''\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with enhanced error handling and reporting.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = \"output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    try:\n",
    "        # Load data\n",
    "        logger.info(\"Loading data...\")\n",
    "        df = pd.read_excel('Updated_Filtered_Dataset.xlsx')\n",
    "        logger.info(f\"Loaded {len(df)} rows of data\")\n",
    "\n",
    "        # Initialize classifier\n",
    "        logger.info(\"Initializing classifier...\")\n",
    "        classifier = MaritimeRiskClassifier()\n",
    "\n",
    "        # Classify dataset\n",
    "        logger.info(\"Starting classification...\")\n",
    "        classified_df = classifier.classify_dataset(df)\n",
    "\n",
    "        # Save results\n",
    "        output_file = os.path.join(output_dir, f'Severity_Output_{timestamp}.xlsx')\n",
    "        classified_df.to_excel(output_file, index=False)\n",
    "        logger.info(f\"Results saved to {output_file}\")\n",
    "\n",
    "        # Generate summary\n",
    "        summary = classified_df['severity_level'].value_counts().sort_index()\n",
    "        logger.info(\"\\nClassification Summary:\")\n",
    "        for level, count in summary.items():\n",
    "            logger.info(f\"Severity Level {level}: {count} incidents ({count/len(classified_df)*100:.1f}%)\")\n",
    "\n",
    "        # Save summary\n",
    "        summary_file = os.path.join(output_dir, f'Summary_{timestamp}.txt')\n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(\"Classification Summary\\n\")\n",
    "            f.write(\"---------------------\\n\")\n",
    "            for level, count in summary.items():\n",
    "                f.write(f\"Severity Level {level}: {count} incidents ({count/len(classified_df)*100:.1f}%)\\n\")\n",
    "\n",
    "        logger.info(f\"Summary saved to {summary_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical error in main: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
